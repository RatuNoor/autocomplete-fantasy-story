{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ver 2. with encoding detector\n"
      ],
      "metadata": {
        "id": "BpQtEoU0ybpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "import os\n",
        "import chardet  # You may need to install the chardet library\n",
        "\n",
        "# Create a folder to store the downloaded books\n",
        "if not os.path.exists('books'):\n",
        "    os.makedirs('books')\n",
        "\n",
        "# Create the output text file with utf-8 encoding\n",
        "output_file = open('fantasy-dataset.txt', 'w', encoding='utf-8')\n",
        "\n",
        "# Read the CSV file with the book links\n",
        "with open('fantasy-link.csv', 'r', encoding='utf-8') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "\n",
        "    # Loop through the links and extract book content\n",
        "    for row in reader:\n",
        "        link = row[0]  # Assuming the link is in the first (and only) column\n",
        "        book_id = link.split('/')[-1]\n",
        "\n",
        "        # Attempt to download the book text from the first form URL\n",
        "        url_form1 = f'https://www.gutenberg.org/files/{book_id}/{book_id}-0.txt'\n",
        "        response = requests.get(url_form1)\n",
        "\n",
        "        # If the first form URL doesn't yield results, try the second form URL\n",
        "        if response.status_code != 200:\n",
        "            url_form2 = f'https://www.gutenberg.org/cache/epub/{book_id}/pg{book_id}.txt'\n",
        "            response = requests.get(url_form2)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            # Decode the content using chardet to detect the encoding\n",
        "            detected_encoding = chardet.detect(response.content)['encoding']\n",
        "            decoded_content = response.content.decode(detected_encoding)\n",
        "\n",
        "            # Encode the content in UTF-8\n",
        "            utf8_encoded_content = decoded_content.encode('utf-8', errors='ignore')\n",
        "\n",
        "            # Write a separator with book ID\n",
        "            output_file.write(f'#####BOOK-ID-{book_id}#####\\n')\n",
        "\n",
        "            # Write the book content to the output text file with utf-8 encoding\n",
        "            output_file.write(utf8_encoded_content.decode('utf-8'))\n",
        "\n",
        "            # Add '#####END-OF-BOOK#####' at the end of each book\n",
        "            output_file.write('\\n#####END-OF-BOOK#####\\n')\n",
        "\n",
        "            print(f'Book {book_id} scraped and saved.')\n",
        "\n",
        "# Close the output text file\n",
        "output_file.close()\n",
        "\n",
        "print('Scraping and text file creation completed.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10zBtfyXx8SW",
        "outputId": "b76f3627-a8e7-424c-c078-130bc946f2b2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Book 55 scraped and saved.\n",
            "Book 1251 scraped and saved.\n",
            "Book 12753 scraped and saved.\n",
            "Book 22566 scraped and saved.\n",
            "Book 10148 scraped and saved.\n",
            "Book 1252 scraped and saved.\n",
            "Book 54 scraped and saved.\n",
            "Book 1152 scraped and saved.\n",
            "Book 5160 scraped and saved.\n",
            "Book 10002 scraped and saved.\n",
            "Book 7477 scraped and saved.\n",
            "Book 831 scraped and saved.\n",
            "Book 8395 scraped and saved.\n",
            "Book 2892 scraped and saved.\n",
            "Book 10806 scraped and saved.\n",
            "Book 10662 scraped and saved.\n",
            "Book 8183 scraped and saved.\n",
            "Book 3261 scraped and saved.\n",
            "Book 964 scraped and saved.\n",
            "Book 832 scraped and saved.\n",
            "Book 7838 scraped and saved.\n",
            "Book 1557 scraped and saved.\n",
            "Book 169 scraped and saved.\n",
            "Book 8129 scraped and saved.\n",
            "Book 420 scraped and saved.\n",
            "Book 5713 scraped and saved.\n",
            "Book 419 scraped and saved.\n",
            "Book 11440 scraped and saved.\n",
            "Book 3055 scraped and saved.\n",
            "Book 1605 scraped and saved.\n",
            "Book 4282 scraped and saved.\n",
            "Book 518 scraped and saved.\n",
            "Book 436 scraped and saved.\n",
            "Book 10745 scraped and saved.\n",
            "Book 8771 scraped and saved.\n",
            "Book 16435 scraped and saved.\n",
            "Book 486 scraped and saved.\n",
            "Book 517 scraped and saved.\n",
            "Book 13486 scraped and saved.\n",
            "Book 2865 scraped and saved.\n",
            "Book 960 scraped and saved.\n",
            "Book 4358 scraped and saved.\n",
            "Book 959 scraped and saved.\n",
            "Book 18328 scraped and saved.\n",
            "Book 604 scraped and saved.\n",
            "Book 13820 scraped and saved.\n",
            "Book 956 scraped and saved.\n",
            "Book 961 scraped and saved.\n",
            "Book 15551 scraped and saved.\n",
            "Book 19959 scraped and saved.\n",
            "Book 13664 scraped and saved.\n",
            "Book 2414 scraped and saved.\n",
            "Book 955 scraped and saved.\n",
            "Book 6050 scraped and saved.\n",
            "Book 6582 scraped and saved.\n",
            "Book 17134 scraped and saved.\n",
            "Book 8778 scraped and saved.\n",
            "Book 2565 scraped and saved.\n",
            "Book 15948 scraped and saved.\n",
            "Book 2060 scraped and saved.\n",
            "Book 958 scraped and saved.\n",
            "Book 10882 scraped and saved.\n",
            "Book 11283 scraped and saved.\n",
            "Book 1267 scraped and saved.\n",
            "Book 1751 scraped and saved.\n",
            "Book 16259 scraped and saved.\n",
            "Book 11639 scraped and saved.\n",
            "Book 1311 scraped and saved.\n",
            "Book 234 scraped and saved.\n",
            "Book 19973 scraped and saved.\n",
            "Book 1077 scraped and saved.\n",
            "Book 12747 scraped and saved.\n",
            "Book 5988 scraped and saved.\n",
            "Book 10041 scraped and saved.\n",
            "Book 19976 scraped and saved.\n",
            "Book 4356 scraped and saved.\n",
            "Book 9663 scraped and saved.\n",
            "Book 9829 scraped and saved.\n",
            "Book 3687 scraped and saved.\n",
            "Book 17973 scraped and saved.\n",
            "Book 11097 scraped and saved.\n",
            "Book 9608 scraped and saved.\n",
            "Book 11752 scraped and saved.\n",
            "Book 9488 scraped and saved.\n",
            "Book 8715 scraped and saved.\n",
            "Scraping and text file creation completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning\n"
      ],
      "metadata": {
        "id": "SkL-q8s_1Xhi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Useless ending page removal"
      ],
      "metadata": {
        "id": "9dYyMONd1Ebz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Read the content of 'fantasy-dataset.txt'\n",
        "with open('fantasy-dataset.txt', 'r', encoding='utf-8') as infile:\n",
        "    content = infile.read()\n",
        "\n",
        "# Define a regular expression pattern to match the desired sections\n",
        "pattern = r'\\*\\*\\* END OF THE PROJECT GUTENBERG EBOOK.*?#####END-OF-BOOK#####'\n",
        "\n",
        "# Use re.sub to remove the matched sections from the content\n",
        "cleaned_content = re.sub(pattern, '#####END-OF-BOOK#####', content, flags=re.DOTALL)\n",
        "\n",
        "# Write the cleaned content back to 'fantasy-dataset.txt'\n",
        "with open('fantasy-dataset.txt', 'w', encoding='utf-8') as outfile:\n",
        "    outfile.write(cleaned_content)\n",
        "\n",
        "print('Text removal completed.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RbYo-x71Dq1",
        "outputId": "cc3432e7-a241-4d30-e6d5-124351a9fca2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text removal completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Useless opening page removal"
      ],
      "metadata": {
        "id": "WDmoW0DJ1sIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Read the content of 'fantasy-dataset.txt'\n",
        "with open('fantasy-dataset.txt', 'r', encoding='utf-8') as infile:\n",
        "    content = infile.read()\n",
        "\n",
        "# Define a regular expression pattern to match and remove the desired sections\n",
        "pattern = r'#####BOOK-ID-(\\d+)#####(.*?)\\*\\*\\* START OF THE PROJECT GUTENBERG EBOOK'\n",
        "\n",
        "# Use re.sub to replace the matched sections with just the \"#####BOOK-ID-{the-id}#####\"\n",
        "cleaned_content = re.sub(pattern, '#####BOOK-ID-\\\\1#####\\n*** START OF THE PROJECT GUTENBERG EBOOK', content, flags=re.DOTALL)\n",
        "\n",
        "# Write the cleaned content back to 'fantasy-dataset.txt'\n",
        "with open('fantasy-dataset.txt', 'w', encoding='utf-8') as outfile:\n",
        "    outfile.write(cleaned_content)\n",
        "\n",
        "print('Text removal completed.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTEhNpoq1vyc",
        "outputId": "dcfaa20d-4f9d-4250-833c-b5e48f7d7385"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text removal completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CH3RyR3C3kRC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}